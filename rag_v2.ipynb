{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhOjil3OD8gp",
        "outputId": "5bd5aaf3-8665-4912-cf46-11194a839454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.11)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.10)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.80)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "bbT12xlwEHDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfLoader = PyPDFLoader(\"1706.03762.pdf\")\n",
        "documents = pdfLoader.load()"
      ],
      "metadata": {
        "id": "DRGRYnWDEXSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "nW4TRBz5EdVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelPath = \"all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings':False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name = modelPath,\n",
        "  model_kwargs = model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "k2VEQA-XEgCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(docs, embeddings)\n",
        "question = \"What is multi-head attention?\"\n",
        "searchDocs = db.similarity_search(question)\n",
        "print(searchDocs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuFiXWm0EqFe",
        "outputId": "f6de7fe8-f631-49c2-9d11-bb183b7732b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
            "where head i= Attention( QWQ\n",
            "i, KWK\n",
            "i, V WV\n",
            "i)\n",
            "Where the projections are parameter matrices WQ\n",
            "i∈Rdmodel×dk,WK\n",
            "i∈Rdmodel×dk,WV\n",
            "i∈Rdmodel×dv\n",
            "andWO∈Rhdv×dmodel.\n",
            "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-step-50K-105b\")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline = pipe,\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 512},\n",
        ")"
      ],
      "metadata": {
        "id": "FMbHvgUWE43W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "awaN9-mjFJC3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=db.as_retriever(),\n",
        "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT,\n",
        "                     \"verbose\":True}\n",
        ")\n",
        "\n",
        "result = qa_chain({\"query\" : question})\n",
        "print(result[\"result\"])"
      ],
      "metadata": {
        "id": "dX2CQp2FFTZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50991de8-4101-4bfb-ec7d-72f97be5a460"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
            "output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\n",
            "where head i= Attention( QWQ\n",
            "i, KWK\n",
            "i, V WV\n",
            "i)\n",
            "Where the projections are parameter matrices WQ\n",
            "i∈Rdmodel×dk,WK\n",
            "i∈Rdmodel×dk,WV\n",
            "i∈Rdmodel×dv\n",
            "andWO∈Rhdv×dmodel.\n",
            "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "\n",
            "Attention Visualizations\n",
            "Input-Input Layer5\n",
            "It\n",
            "is\n",
            "in\n",
            "this\n",
            "spirit\n",
            "that\n",
            "a\n",
            "majority\n",
            "of\n",
            "American\n",
            "governments\n",
            "have\n",
            "passed\n",
            "new\n",
            "laws\n",
            "since\n",
            "2009\n",
            "making\n",
            "the\n",
            "registration\n",
            "or\n",
            "voting\n",
            "process\n",
            "more\n",
            "difficult\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "It\n",
            "is\n",
            "in\n",
            "this\n",
            "spirit\n",
            "that\n",
            "a\n",
            "majority\n",
            "of\n",
            "American\n",
            "governments\n",
            "have\n",
            "passed\n",
            "new\n",
            "laws\n",
            "since\n",
            "2009\n",
            "making\n",
            "the\n",
            "registration\n",
            "or\n",
            "voting\n",
            "process\n",
            "more\n",
            "difficult\n",
            ".\n",
            "<EOS>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
            "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
            "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
            "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
            "13\n",
            "\n",
            "Scaled Dot-Product Attention\n",
            " Multi-Head Attention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attention layers running in parallel.\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
            "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
            "values.\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
            "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
            "the matrix of outputs as:\n",
            "Attention( Q, K, V ) = softmax(QKT\n",
            "√dk)V (1)\n",
            "\n",
            "between any two positions in the network. Convolutional layers are generally more expensive than\n",
            "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
            "considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\n",
            "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
            "the approach we take in our model.\n",
            "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
            "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
            "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
            "and semantic structure of the sentences.\n",
            "5 Training\n",
            "This section describes the training regime for our models.\n",
            "5.1 Training Data and Batching\n",
            "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.2 Training\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.3 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.4 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.5 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.6 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.7 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.8 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.9 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.10 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.11 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.12 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.13 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head attention is a technique that allows the model to learn to attend to\n",
            "information from different representations at different positions.\n",
            "5.14 Model Architecture\n",
            "We use the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "Question: What is multi-head attention?\n",
            "Helpful Answer: Multi-head\n"
          ]
        }
      ]
    }
  ]
}